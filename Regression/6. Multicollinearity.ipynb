{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b84b5a9b",
   "metadata": {},
   "source": [
    "# *Multicollinearity*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec6889e",
   "metadata": {},
   "source": [
    "**Correlation**: X --> y ~ 0.95 (Say)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7428202b",
   "metadata": {},
   "source": [
    "If there is x unit increase in X then there is 95% chance that y will also increase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb748f07",
   "metadata": {},
   "source": [
    "1. If X has high correlation with y then the model will be good.\n",
    "    - **What will happen if the features among themselves has a good correlation?**\n",
    "2. To measure relationship between two features we have correlation. \n",
    "    - **But how to measure correlation among multiple features** ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f18f0c0",
   "metadata": {},
   "source": [
    "- **Correlation among features itself**\n",
    "    - In terms of variation X1 & X2 almost same (if corr b/w X1 , X2 = 0.95%)\n",
    "    - Then in such case when X1 ~ X2, the interpretation of MLR becomes tough.\n",
    "    - Because which features is contributing more to y will be tough to decide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db8b457",
   "metadata": {},
   "source": [
    "- **Measure for correlation among multiple features**\n",
    "    - **Collinearity** : When two features are linearly associated (high correlation) and they are used to predict target variable.\n",
    "    - **Multicollinearity** : When a feature exhibits a linear relationship with more than two variables.\n",
    "\n",
    "- **Why multicollinearity is a concern?**\n",
    "    - Affects interpretation\n",
    "    - Increase computations\n",
    "        - If x1 = x2, then why to use both the features in building the model. It will increase the computation time.\n",
    "    - It increases the overfitting\n",
    "        - If x1 ~ x2 almost same, then pattern will be memorised, that's why it leds to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c47b9f9",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe8e5a1",
   "metadata": {},
   "source": [
    "1. **VIF (Variation Inflation Factor)** and **Drop Feature**\n",
    "2. **RFE (Recursive Feature Elimination)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe90cf1",
   "metadata": {},
   "source": [
    "###  VIF\n",
    "\n",
    "\n",
    "It is a measure of amount of multicollinearity in Regression.\n",
    "- VIF >= 10\n",
    "    - When features have VIF >= 10, then we say high multicollinearity, then drop the features one by one until all the features has VIF < 10.\n",
    "\n",
    "\n",
    "### RFE\n",
    "\n",
    "It will make a model with all features (Say 1000), \n",
    "- Start applying one by one the least important features.\n",
    "    - unitl we get desired number of features are left.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc476086",
   "metadata": {},
   "source": [
    "## Two kinds of Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88229af7",
   "metadata": {},
   "source": [
    "### 1. Data Based Collinearity\n",
    "\n",
    "Present in data itself.\n",
    "\n",
    "### 2. Structured Multicollinearity\n",
    "\n",
    "Caused due to new features from existing features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d23b474",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
