{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d645314",
   "metadata": {},
   "source": [
    "# Evaluation Metrics for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ff4cda",
   "metadata": {},
   "source": [
    "1. Confusion Matrix\n",
    "2. Accuracy / Missclassification Rate\n",
    "3. Precision\n",
    "4. Recall\n",
    "5. F- Beta Score\n",
    "6. True Position Rate (Senstivity)\n",
    "7. False Position Rate\n",
    "8. True Negative Rate (Specificity)\n",
    "9. ROC-AUC\n",
    "10. Precision-Recall / Senstivity-Specificity Trade-off\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb297af",
   "metadata": {},
   "source": [
    "## 1. Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ef697",
   "metadata": {},
   "source": [
    "A **confusion matrix** is a 2x2 table used to check how good a classification model is.\n",
    "It shows where the model is correct and where it is confused.\n",
    "\n",
    "Imagine a binary classification (Yes/No).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6634b",
   "metadata": {},
   "source": [
    "- **Confusion Matrix (Markdown Table)**\n",
    "\n",
    "| Predicted \\ Actual | Actual 1 (Positive) | Actual 0 (Negative) |\n",
    "|--------------------|----------------------|----------------------|\n",
    "| **Predicted 1**    | TP (True Positive)   | FP (False Positive)  |\n",
    "| **Predicted 0**    | FN (False Negative)  | TN (True Negative)   |\n",
    "\n",
    "\n",
    "### TP, FP, FN, TN Explanation Table\n",
    "\n",
    "| Term | Full Form        | Model Prediction | Actual Value | Meaning                  |\n",
    "|------|------------------|------------------|--------------|---------------------------|\n",
    "| TP   | True Positive     | 1                | 1            | Correct positive prediction |\n",
    "| FP   | False Positive    | 1                | 0            | Incorrect positive prediction |\n",
    "| FN   | False Negative    | 0                | 1            | Model missed a real positive   |\n",
    "| TN   | True Negative     | 0                | 0            | Correct negative prediction |\n",
    "\n",
    "\n",
    "\n",
    "### Reading of Matrix\n",
    "\n",
    "- **TP high** :  model is good at catching positives\n",
    "- **TN high** : model is good at catching negatives\n",
    "- **FP high** : model gives too many false alarms\n",
    "- **FN high** : model misses important cases\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c40730",
   "metadata": {},
   "source": [
    "## 2. Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aabef61",
   "metadata": {},
   "source": [
    "Accuracy tells how much the model is correct overall.\n",
    "It measures the percentage of total correct predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c56200",
   "metadata": {},
   "source": [
    "- **Formula**\n",
    "- Accuracy = `(TP + TN) / (TP + FP + FN + TN)`\n",
    "\n",
    "- Where:\n",
    "    - TP = True Positive\n",
    "    - TN = True Negative\n",
    "    - FP = False Positive\n",
    "    - FN = False Negative\n",
    "\n",
    "- **Interpretation**\n",
    "    - High Accuracy : Model is performing well overall.\n",
    "    - Low Accuracy : Model is making many mistakes.\n",
    "    - Works best when dataset is balanced (equal positives & negatives).\n",
    "\n",
    "- **Example Understanding**\n",
    "- If model made:\n",
    "    - TP = 30  \n",
    "    - TN = 50  \n",
    "    - FP = 10  \n",
    "    - FN = 10  \n",
    "\n",
    "- Then:\n",
    "    - Accuracy = (30 + 50) / (30 + 50 + 10 + 10)  \n",
    "    - Accuracy = 80 / 100  \n",
    "Accuracy = **0.80 (80%)**\n",
    "\n",
    "\n",
    "Accuracy is correct classification rate.\n",
    "- Opposite of Accuracy/ Classification Rate : Missclassification Rate = `All wrong Prediction / Total datapoints`\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e5e447",
   "metadata": {},
   "source": [
    "## 3. Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddce940",
   "metadata": {},
   "source": [
    "- **Formula**\n",
    "Precision = `TP / (TP + FP)`\n",
    "\n",
    "- **Meaning of Precision Components**\n",
    "\n",
    "| Term | Full Form       | Meaning                                      |\n",
    "|------|------------------|----------------------------------------------|\n",
    "| TP   | True Positive     | Correctly predicted positive cases           |\n",
    "| FP   | False Positive    | Incorrectly predicted positive cases         |\n",
    "\n",
    "\n",
    "- **What Precision Tells**\n",
    "    - Out of all **predicted positives**, how many were actually positive?\n",
    "    - Focuses on *positive prediction correctness*.\n",
    "    - High Precision = fewer false alarms.\n",
    "\n",
    "\n",
    "- **Example**\n",
    "    - If TP = 45 and FP = 5:\n",
    "\n",
    "    - Precision = 45 / (45 + 5)  \n",
    "    - Precision = 45 / 50  \n",
    "    - Precision = **0.90 (90%)**\n",
    "\n",
    "- **When Precision Matters?**\n",
    "    - When false positives are dangerous\n",
    "  (like disease diagnosis, fraud alert, spam detection).\n",
    "    - Whan all class 1 dps are correctly predicted and all class 0 dps are wrongly predicted.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa98ca1",
   "metadata": {},
   "source": [
    "## 4. Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8492e5",
   "metadata": {},
   "source": [
    "- **Formula**\n",
    "Recall = `TP / (TP + FN)`\n",
    "\n",
    "- **Meaning of Recall Components**\n",
    "\n",
    "| Term | Full Form        | Meaning                                      |\n",
    "|------|-------------------|----------------------------------------------|\n",
    "| TP   | True Positive     | Correctly predicted positive cases           |\n",
    "| FN   | False Negative    | Model missed actual positive cases           |\n",
    "\n",
    "- **What Recall Tells**\n",
    "    - Out of all **actual positives**, how many did the model correctly catch?\n",
    "    - Measures the model’s ability to **find positives**.\n",
    "    - High Recall = model rarely misses positives.\n",
    "\n",
    "- **Example**\n",
    "    - If TP = 30 and FN = 20:\n",
    "\n",
    "    - Recall = 30 / (30 + 20)  \n",
    "    - Recall = 30 / 50  \n",
    "    - Recall = **0.60 (60%)**\n",
    "\n",
    "- **When Recall Matters?**\n",
    "    - When missing a positive is dangerous  \n",
    "  (disease detection, fraud detection, cancer detection).\n",
    "\n",
    "\n",
    "- **Note** :\n",
    "    - Both **Precision** and **Recall** have seen in terms of class 1.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25177d19",
   "metadata": {},
   "source": [
    "## 5. F-Beta Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97cacb9",
   "metadata": {},
   "source": [
    "- **Definition**\n",
    "\n",
    "F-Beta Score is a metric that combines **Precision** and **Recall**.\n",
    "It allows you to decide which one is more important by adjusting **β (beta)**.\n",
    "\n",
    "    - If β > 1 : Give more importance to **Recall**\n",
    "    - If β < 1 : Give more importance to **Precision**\n",
    "    - If β = 1 : It becomes **F1 Score** (balanced)\n",
    "\n",
    "- **Formula**\n",
    "\n",
    "Fβ = (1 + β²) * ( (Precision * Recall) / ( (β² * Precision) + Recall ) )\n",
    "\n",
    "\n",
    "- **Meaning of Beta (β)**\n",
    "\n",
    "| β Value | Priority | Meaning                                      |\n",
    "|---------|----------|----------------------------------------------|\n",
    "| β < 1   | Precision | You care more about minimizing false positives |\n",
    "| β = 1   | Balanced  | Precision and Recall equally important (F1)   |\n",
    "| β > 1   | Recall    | You care more about minimizing false negatives |\n",
    "\n",
    "\n",
    "- **Component Terms**\n",
    "\n",
    "| Term       | Meaning                                      |\n",
    "|------------|----------------------------------------------|\n",
    "| Precision  | TP / (TP + FP) — correctness of predicted positives |\n",
    "| Recall     | TP / (TP + FN) — coverage of actual positives     |\n",
    "\n",
    "- **Example**\n",
    "Suppose:\n",
    "    - Precision = 0.80  \n",
    "    - Recall = 0.60  \n",
    "    - β = 2 → Recall is more important\n",
    "\n",
    "- F2 = (1 + 2²) * ( (0.80 * 0.60) / ( (2² * 0.80) + 0.60 ) )  \n",
    "- F2 = 5 * (0.48 / (3.2 + 0.6))  \n",
    "- F2 = 5 * (0.48 / 3.8)  \n",
    "- F2 ≈ **0.63**\n",
    "\n",
    "- **When F-Beta is Useful?**\n",
    "\n",
    "| Scenario                          | Metric Focus                  |\n",
    "|-----------------------------------|-------------------------------|\n",
    "| Missing positives is dangerous    | High Recall (β > 1)           |\n",
    "| False alarms are dangerous        | High Precision (β < 1)        |\n",
    "| Balanced tasks                    | β = 1 (F1 Score)              |\n",
    "\n",
    "- **Quick Summary**\n",
    "- F-Beta adjusts the balance between Precision and Recall.  \n",
    "- β controls “which metric matters more”.  \n",
    "- Useful when dataset is imbalanced.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46653b9a",
   "metadata": {},
   "source": [
    "## 6. True Positive Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a051914e",
   "metadata": {},
   "source": [
    "- **Other Names**\n",
    "    - TPR = Recall\n",
    "    - TPR = Sensitivity\n",
    "    - TPR = Hit Rate\n",
    "\n",
    "All three mean the same thing.\n",
    "\n",
    "\n",
    "- **Formula**\n",
    "`TPR = TP / (TP + FN)`\n",
    "\n",
    "\n",
    "- **Meaning of Terms**\n",
    "\n",
    "| Term | Full Form        | Meaning                                      |\n",
    "|------|-------------------|----------------------------------------------|\n",
    "| TP   | True Positive     | Model correctly predicted positive cases     |\n",
    "| FN   | False Negative    | Model missed actual positive cases           |\n",
    "\n",
    "\n",
    "- **What TPR Tells**\n",
    "    - Out of **all actual positive cases**, how many the model successfully caught.\n",
    "    - Measures **how sensitive** the model is to detecting positives.\n",
    "\n",
    "    - High TPR : Model rarely misses positives  \n",
    "    - Low TPR : Model misses many actual positives\n",
    "\n",
    "- **Example**\n",
    "- If TP = 45 and FN = 5:\n",
    "\n",
    "    - TPR = 45 / (45 + 5)  \n",
    "    - TPR = 45 / 50  \n",
    "    - TPR = **0.90 (90%)**\n",
    "\n",
    "- **When TPR Matters**\n",
    "    - Disease detection\n",
    "    - Fraud detection\n",
    "    - Any case where **missing a positive is dangerous**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23b8c64",
   "metadata": {},
   "source": [
    "## 7. False Positive Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089f1426",
   "metadata": {},
   "source": [
    "- **Other Names**\n",
    "    - FPR = Fall-Out\n",
    "    - FPR = False Alarm Rate\n",
    "\n",
    "All refer to the same concept.\n",
    "\n",
    "- **Formula**\n",
    "`FPR = FP / (FP + TN)`\n",
    "\n",
    "- **Meaning of Terms**\n",
    "\n",
    "| Term | Full Form        | Meaning                                      |\n",
    "|------|-------------------|----------------------------------------------|\n",
    "| FP   | False Positive    | Model predicted positive but was wrong       |\n",
    "| TN   | True Negative     | Model correctly predicted negative           |\n",
    "\n",
    "- **What FPR Tells**\n",
    "    - Out of all **actual negative cases**, how many the model incorrectly marked as positive.\n",
    "    - Measures how often the model **raises false alarms**.\n",
    "\n",
    "    - High FPR : Model gives too many false positives  \n",
    "    - Low FPR : Model avoids false alarms well\n",
    "\n",
    "- **Example**\n",
    "- If FP = 10 and TN = 90:\n",
    "\n",
    "    - FPR = 10 / (10 + 90)  \n",
    "    - FPR = 10 / 100  \n",
    "    - FPR = **0.10 (10%)**\n",
    "\n",
    "- **When FPR Matters?**\n",
    "    - Spam filters (don’t mark real emails as spam)\n",
    "    - Security systems (don’t trigger alerts unnecessarily)\n",
    "    - Any scenario where false alarms cause problems\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbf09ab",
   "metadata": {},
   "source": [
    "## 8. True Negative Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37748146",
   "metadata": {},
   "source": [
    "- **Formula**\n",
    "`TNR = TN / (TN + FP)`\n",
    "\n",
    "- **Meaning of Terms**\n",
    "\n",
    "| Term | Full Form        | Meaning                                        |\n",
    "|------|-------------------|------------------------------------------------|\n",
    "| TN   | True Negative     | Model correctly predicted negative cases       |\n",
    "| FP   | False Positive    | Model predicted positive but was wrong         |\n",
    "\n",
    "\n",
    "- **What TNR Tells**\n",
    "    - Out of all **actual negative cases**, how many were correctly predicted as negative.\n",
    "    - Measures the model’s ability to **avoid false alarms**.\n",
    "\n",
    "    - High TNR : Model rarely gives false positives  \n",
    "    - Low TNR → Model keeps raising unnecessary alerts\n",
    "\n",
    "- **Example**\n",
    "- If TN = 90 and FP = 10:\n",
    "\n",
    "    - TNR = 90 / (90 + 10)  \n",
    "    - TNR = 90 / 100  \n",
    "    - TNR = **0.90 (90%)**\n",
    "- **Other Names**\n",
    "    - Specificity\n",
    "    - True Negative Proportion\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67f0599",
   "metadata": {},
   "source": [
    "## 9. False Negative Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a2ece8",
   "metadata": {},
   "source": [
    "- **Formula**\n",
    "`FNR = FN / (TP + FN)`\n",
    "\n",
    "\n",
    "- **Meaning of Terms**\n",
    "\n",
    "| Term | Full Form        | Meaning                                      |\n",
    "|------|-------------------|----------------------------------------------|\n",
    "| FN   | False Negative    | Model predicted negative but it was positive |\n",
    "| TP   | True Positive     | Model correctly predicted positive           |\n",
    "\n",
    "\n",
    "- **What FNR Tells**\n",
    "    - Out of all **actual positive cases**, how many the model **failed to detect**.\n",
    "    - Measures how often the model **misses positive cases**.\n",
    "\n",
    "    - High FNR : Model misses many positives  \n",
    "    - Low FNR : Model catches positives well\n",
    "\n",
    "- **Example**\n",
    "- If FN = 20 and TP = 80:\n",
    "\n",
    "    - FNR = 20 / (80 + 20)  \n",
    "    - FNR = 20 / 100  \n",
    "    - FNR = **0.20 (20%)**\n",
    "\n",
    "\n",
    "- **Other Names**\n",
    "    - Miss Rate\n",
    "    - False Negative Proportion\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaae8b7c",
   "metadata": {},
   "source": [
    "## 10. ROC-AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dd309e",
   "metadata": {},
   "source": [
    "- **ROC**\n",
    "    - ROC = Receiver Operating Characteristic Curve\n",
    "\n",
    "    - It is a graph that shows the performance of a classification model at different threshold values.\n",
    "\n",
    "\n",
    "- **ROC Curve Axes**\n",
    "\n",
    "| Axis | Full Form               | What It Represents                     |\n",
    "|------|--------------------------|-----------------------------------------|\n",
    "| X    | False Positive Rate (FPR) | Wrong positives (FP / (FP + TN))        |\n",
    "| Y    | True Positive Rate (TPR)  | Correct positives (TP / (TP + FN))      |\n",
    "\n",
    "- ROC curve = plot of **TPR (y-axis)** vs **FPR (x-axis)**.\n",
    "\n",
    "\n",
    "- **What is AUC?**\n",
    "    - AUC = Area Under the ROC Curve\n",
    "\n",
    "    - It measures **how well the model separates classes**.\n",
    "\n",
    "\n",
    "- **AUC Interpretation**\n",
    "\n",
    "| AUC Value | Model Meaning                                      |\n",
    "|-----------|-----------------------------------------------------|\n",
    "| 1.0       | Perfect classifier                                  |\n",
    "| 0.9 - 1.0 | Excellent                                           |\n",
    "| 0.8 - 0.9 | Good                                               |\n",
    "| 0.7 - 0.8 | Fair                                               |\n",
    "| 0.5 - 0.7 | Poor                                               |\n",
    "| 0.5       | No skill (same as random guessing)                  |\n",
    "| < 0.5     | Worse than random                                   |\n",
    "\n",
    "- Higher AUC = Better model performance.\n",
    "\n",
    "\n",
    "- **Why ROC–AUC is Useful?**\n",
    "\n",
    "    - Works well even for **imbalanced datasets**  \n",
    "    - Looks at **all threshold values**, not just one  \n",
    "    - Helps compare multiple models easily  \n",
    "\n",
    "\n",
    "- **Key Terms Used in ROC**\n",
    "\n",
    "| Term | Full Form            | Formula                      |\n",
    "|------|-----------------------|-------------------------------|\n",
    "| TPR  | True Positive Rate    | TP / (TP + FN) (Recall)       |\n",
    "| FPR  | False Positive Rate   | FP / (FP + TN)                |\n",
    "| TNR  | True Negative Rate    | TN / (TN + FP)                |\n",
    "| FNR  | False Negative Rate   | FN / (TP + FN)                |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9600b98f",
   "metadata": {},
   "source": [
    "## 11. Precision-Recall Trade Off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a61fea0",
   "metadata": {},
   "source": [
    "- **What is the Trade-off?**\n",
    "    - Precision and Recall cannot increase together all the time.\n",
    "    - When you try to increase one, the other often decreases.\n",
    "\n",
    "- *Reason:*\n",
    "    - Both depend on the **classification threshold**.\n",
    "    - Changing threshold changes TP, FP, FN → which affects Precision & Recall differently.\n",
    "\n",
    "\n",
    "- **Effect of Threshold on Precision & Recall**\n",
    "\n",
    "| Threshold | Precision Effect                     | Recall Effect                          |\n",
    "|-----------|---------------------------------------|-----------------------------------------|\n",
    "| High      | Higher Precision                      | Lower Recall                            |\n",
    "| Low       | Lower Precision                       | Higher Recall                           |\n",
    "\n",
    "\n",
    "- **Why This Happens?**\n",
    "\n",
    "| Metric     | Increases When…                                 |\n",
    "|------------|--------------------------------------------------|\n",
    "| Precision  | You predict fewer positives → fewer false alarms |\n",
    "| Recall     | You predict more positives → catch more positives |\n",
    "\n",
    "    So:\n",
    "        - Predict **more positives** : Recall ↑ , Precision ↓  \n",
    "        - Predict **fewer positives** : Precision ↑ , Recall ↓  \n",
    "\n",
    "\n",
    "- **Example Understanding**\n",
    "    - If you want **high precision** (fewer false positives):\n",
    "        - Model becomes very strict.\n",
    "        - Predicts positive only when very sure.\n",
    "        - Misses some real positives → Recall drops.\n",
    "\n",
    "    - If you want **high recall** (catch all positives):\n",
    "        - Model becomes generous.\n",
    "        - Predicts positive more often.\n",
    "        - Creates more false positives → Precision drops.\n",
    "\n",
    "- **When to Prefer What?**\n",
    "\n",
    "| Situation                                 | Focus On     | Reason                                      |\n",
    "|--------------------------------------------|--------------|---------------------------------------------|\n",
    "| Disease detection                          | Recall ↑     | Missing a positive is dangerous             |\n",
    "| Fraud detection                            | Recall ↑     | Better to investigate extra cases           |\n",
    "| Spam detection                             | Precision ↑  | Don't mark real emails as spam              |\n",
    "| Job candidate filtering                    | Precision ↑  | Only select strong candidates               |\n",
    "\n",
    "\n",
    "- **Summary**\n",
    "    - Precision and Recall fight over threshold.  \n",
    "    - Increase one → the other often decreases.  \n",
    "    - Choose based on what mistake is more expensive:\n",
    "        - **False Positive expensive : Precision**\n",
    "        - **False Negative expensive : Recall**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107575e3",
   "metadata": {},
   "source": [
    "# Summary of Evaluation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a786bd7",
   "metadata": {},
   "source": [
    "## MEGA SUMMARY TABLE — ALL CLASSIFICATION METRICS\n",
    "\n",
    "| Metric | Full Form / Other Names | Formula | Uses / What It Tells | High Value Means | Low Value Means |\n",
    "|--------|---------------------------|---------|------------------------|-------------------|------------------|\n",
    "| **TPR** | True Positive Rate / Recall / Sensitivity | TP / (TP + FN) | How many actual positives the model catches | Few positives missed | Many positives missed |\n",
    "| **FNR** | False Negative Rate / Miss Rate | FN / (TP + FN) | How many positives the model failed to detect | Many missed positives | Few missed positives |\n",
    "| **TNR** | True Negative Rate / Specificity | TN / (TN + FP) | How well the model avoids false alarms | Few false positives | Many false positives |\n",
    "| **FPR** | False Positive Rate / Fall-Out | FP / (FP + TN) | How often model raises false alarms | Many false positives | Few false positives |\n",
    "| **Precision** | Positive Predictive Value | TP / (TP + FP) | How many predicted positives were correct | Accurate positive predictions | Many wrong positive predictions |\n",
    "| **Accuracy** | Overall correctness | (TP + TN) / (TP + FP + FN + TN) | How often model is right overall | Good general performance | Many overall mistakes |\n",
    "| **F1 Score** | Harmonic mean of Precision & Recall | 2 * (P * R) / (P + R) | Balances precision and recall | Balanced P & R | Imbalance between P & R |\n",
    "| **F-Beta** | Weighted F-score | (1+β²) * (PR / (β²P + R)) | Choose P or R importance | Controlled balance | Wrong balance |\n",
    "| **ROC Curve** | Receiver Operating Characteristic | TPR vs FPR plot | Performance at all thresholds | Better separation | Poor separation |\n",
    "| **AUC** | Area Under ROC Curve | Area value | How well model separates classes | Excellent classifier | Bad classifier |\n",
    "\n",
    "\n",
    "\n",
    "## QUICK MAPPING OF TERMS\n",
    "\n",
    "| Term | Meaning |\n",
    "|------|---------|\n",
    "| TP | Correct positive prediction |\n",
    "| TN | Correct negative prediction |\n",
    "| FP | Wrong positive prediction |\n",
    "| FN | Missed positive prediction |\n",
    "\n",
    "\n",
    "## MEMORY CHEATSHEET (Super Short)\n",
    "\n",
    "- **Precision** : Out of predicted positives, how many are real  \n",
    "- **Recall (TPR)** : Out of actual positives, how many caught  \n",
    "- **TNR (Specificity)** : Out of actual negatives, how many caught  \n",
    "- **FPR** : Out of actual negatives, how many wrongly marked positive  \n",
    "- **FNR** : Out of actual positives, how many missed  \n",
    "- **F1** : Balance of Precision + Recall  \n",
    "- **AUC** : Overall class separation quality  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
